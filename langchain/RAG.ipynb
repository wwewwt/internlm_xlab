{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecac1a38-6d97-48d0-8592-ebe6b52fafe7",
   "metadata": {},
   "source": [
    "# 加载环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb37d99-225d-4a4a-94bf-ea204a7de878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# 首先导入所需第三方库\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# 加载嵌入模型\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "# 建立索引\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a8a0e-fce3-4fc8-aa88-fbe44c79c152",
   "metadata": {},
   "source": [
    "# 配置LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c71ffda-a7c2-4b4e-8713-6ddf4db2842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__f438da3457f5446db8509ca0e0ee48f3\"  # Update to your API key\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1cbb64-cc1b-4d4a-95e7-7e655f8c0e91",
   "metadata": {},
   "source": [
    "# 自定义大模型 Internlm-chat-7b-finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2312290-37ee-4875-b7ef-059271eb1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternLM_LLM(LLM):\n",
    "    # 基于本地 InternLM 自定义 LLM 类\n",
    "    tokenizer : AutoTokenizer = None\n",
    "    model: AutoModelForCausalLM = None\n",
    "\n",
    "    def __init__(self, model_path :str):\n",
    "        # model_path: InternLM 模型路径\n",
    "        # 从本地初始化模型\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(torch.bfloat16).cuda()\n",
    "        self.model = self.model.eval()\n",
    "        print(\"完成本地模型的加载\")\n",
    "\n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        # 重写调用函数\n",
    "        system_prompt = \"\"\"You are an AI assistant whose name is InternLM (书生·浦语).\n",
    "        - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.\n",
    "        - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [(system_prompt, '')]\n",
    "        response, history = self.model.chat(self.tokenizer, prompt , history=messages)\n",
    "        return response\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"InternLM2\"\n",
    "\n",
    "# 加载大模型\n",
    "#llm = InternLM_LLM(\"/root/share/model_repos/internlm2-chat-7b\")\n",
    "llm = InternLM_LLM(\"/root/project/bisai2/sft-guanfang-self/merged_7b_e10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb17c8f",
   "metadata": {},
   "source": [
    "## （可选）离线构建检索库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d42d6",
   "metadata": {},
   "source": [
    "### 定义加载知识库文件的工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb310c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文件路径函数\n",
    "def get_files(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    file_list = []\n",
    "    for filepath, dirnames, filenames in os.walk(dir_path):\n",
    "        # os.walk 函数将递归遍历指定文件夹\n",
    "        for filename in filenames:\n",
    "            # 通过后缀名判断文件类型是否满足要求\n",
    "            if filename.endswith(\".md\"):\n",
    "                # 如果满足要求，将其绝对路径加入到结果列表\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "            elif filename.endswith(\".txt\"):\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "            elif filename.endswith(\".csv\"):\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "    return file_list\n",
    "\n",
    "# 加载文件函数\n",
    "def get_text(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    # 首先调用上文定义的函数得到目标文件路径列表\n",
    "    file_lst = get_files(dir_path)\n",
    "    # docs 存放加载之后的纯文本对象\n",
    "    docs = []\n",
    "    # 遍历所有目标文件\n",
    "    for one_file in tqdm(file_lst):\n",
    "        file_type = one_file.split('.')[-1]\n",
    "        if file_type == 'md':\n",
    "            loader = UnstructuredMarkdownLoader(one_file)\n",
    "        elif file_type == 'txt':\n",
    "            loader = TextLoader(one_file)\n",
    "        elif file_type == 'csv':\n",
    "            loader = CSVLoader(one_file)\n",
    "        else:\n",
    "            # 如果是不符合条件的文件，直接跳过\n",
    "            continue\n",
    "        docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845a41b",
   "metadata": {},
   "source": [
    "## 准备知识库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01aad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#在question 的开头增加时间\n",
    "\n",
    "#官方数据制作的知识库\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_excel('../data/gong_gao_ce_ping.xls', dtype=str)\n",
    "#system_value = \"你是上市公司的董秘，你乐于助人，诚实无害，你竭诚为投资者解答关于公司运营、财务状况、投资者关系等方面的问题。\"\n",
    "train_data = []\n",
    "output_dir = './raw_txt_knowledge'\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        question = f\"{row['评测问题']}\"\n",
    "        answer = row['答案']\n",
    "        if question is None or question == \"\":\n",
    "            print(row)\n",
    "            continue\n",
    "        if answer is None or answer == \"\":\n",
    "            print(row)\n",
    "            continue\n",
    "        \n",
    "        conversation = f\"问：{question} \\n答：{answer}\"\n",
    "    \n",
    "        with open(f'{output_dir}/gong_gao_ce_ping_{index}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(conversation)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(row)\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "print(f\"Conversion complete. Output written to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddf961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大模型生成的问答对知识库\n",
    "import json\n",
    "with open(\"../data/rag_qa_extend.jsonl\", 'r', encoding='utf-8') as file:\n",
    "    json_str = file.read()\n",
    "    train_json = json.loads(json_str)\n",
    "    index = 0\n",
    "    for qa in train_json:\n",
    "        question = qa['conversation'][0][\"input\"]\n",
    "        answer   = qa['conversation'][0][\"output\"]\n",
    "        if question is None or question == \"\":\n",
    "            print(row)\n",
    "            continue\n",
    "        if answer is None or answer == \"\":\n",
    "            print(row)\n",
    "            continue\n",
    "        conversation = f\"问：{question} \\n答：{answer}\"\n",
    "        \n",
    "        with open(f'{output_dir}/qa_extend_{index}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(conversation)\n",
    "        index +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57283866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓取的数据建立的知识库\n",
    "# 大模型生成的问答对知识库\n",
    "import json\n",
    "with open(\"../data/train.jsonl\", 'r', encoding='utf-8') as file:\n",
    "    json_str = file.read()\n",
    "    train_json = json.loads(json_str)\n",
    "    index = 0\n",
    "    for qa in train_json:\n",
    "        question = qa['conversation'][0][\"input\"]\n",
    "        answer   = qa['conversation'][0][\"output\"]\n",
    "        if question is None or question == \"\":\n",
    "            print(qa)\n",
    "            continue\n",
    "        if answer is None or answer == \"\":\n",
    "            print(qa)\n",
    "            continue\n",
    "        conversation = f\"问：{question} \\n答：{answer}\"\n",
    "        \n",
    "        with open(f'{output_dir}/train_{index}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(conversation)\n",
    "        index +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf49fec",
   "metadata": {},
   "source": [
    "## 创建检索库(如果没有)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b23086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知识库文件保存的目录\n",
    "tar_dir = [\n",
    "    #\"/root/project/TinyRAG/data/2024-01-23_10-07-23\",\n",
    "    #\"/root/project/bisai2/data/vector_db_txt\",\n",
    "    \"./raw_txt_knowledge\",\n",
    "]\n",
    "\n",
    "# 加载目标文件\n",
    "docs = []\n",
    "for dir_path in tar_dir:\n",
    "    docs.extend(get_text(dir_path))\n",
    "\n",
    "\n",
    "# 对文本进行分块\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=150)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 加载embedding\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/root/model/jinaai/jina-embeddings-v2-base-zh\")\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"/root/data/model/sentence-transformer\")\n",
    "\n",
    "\n",
    "vectordb = FAISS.from_documents(documents, embeddings)\n",
    "# 将加载的向量数据库持久化到磁盘上\n",
    "vectordb.save_local('./vectordb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304143f",
   "metadata": {},
   "source": [
    "## 创建检索库(如果有)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载已有数据库\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/root/model/jinaai/jina-embeddings-v2-base-zh\")\n",
    "vectordb = FAISS.load_local('./vectordb', embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ddfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个链，该链接受问题和检索到的文档并生成答案\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"使用以上下文来回答用户的问题。总是使用中文回答。\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "如果给定的上下文无法让你做出回答，请对回答的内容以“*”开头，然后根据你的理解作答。\n",
    "有用的回答: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "#对于给定的问题，我们可以使用检索器动态选择最相关的文档并将其传递进去\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# 返回结果\n",
    "response = retrieval_chain.invoke({\"input\": \"上汽集团有多少车辆搭载了德威系列的发动机\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80cb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5ee47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050db943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57623596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bfbbdcb-50de-4e69-bf15-4e4ca8b82641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对不起，我无法回答您的问题，因为我无法找到相关信息。如果您有其他问题，我会很乐意为您提供帮助。 \n"
     ]
    }
   ],
   "source": [
    "# 建立一个链，该链接受问题和检索到的文档并生成答案\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"使用以上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "如果给定的上下文无法让你做出回答，请对回答的内容以“*”开头，然后根据你的理解作答。\n",
    "有用的回答: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "#对于给定的问题，我们可以使用检索器动态选择最相关的文档并将其传递进去\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# 返回结果\n",
    "response = retrieval_chain.invoke({\"input\": \"上汽集团的董事长是谁\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f9238-fdc7-43b6-a9d7-f9a786c65cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  提取问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30b239a-2e8d-4863-94f8-8593dbf3081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好！我是一个名叫书生·浦语的AI助手，很高兴为您服务。我致力于通过自然语言处理和深度学习技术，帮助解答您的问题，提供信息和建议。如果您有任何疑问或需要帮助，请随时告诉我。 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"你好\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6eced6-7475-4e63-a087-f7a706502475",
   "metadata": {},
   "source": [
    "# 离线构建检索库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12bbcd0e-d6ab-4bbb-9888-b6dd6367d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文件路径函数\n",
    "def get_files(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    file_list = []\n",
    "    for filepath, dirnames, filenames in os.walk(dir_path):\n",
    "        # os.walk 函数将递归遍历指定文件夹\n",
    "        for filename in filenames:\n",
    "            # 通过后缀名判断文件类型是否满足要求\n",
    "            if filename.endswith(\".md\"):\n",
    "                # 如果满足要求，将其绝对路径加入到结果列表\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "            elif filename.endswith(\".txt\"):\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "            elif filename.endswith(\".csv\"):\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "    return file_list\n",
    "\n",
    "# 加载文件函数\n",
    "def get_text(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    # 首先调用上文定义的函数得到目标文件路径列表\n",
    "    file_lst = get_files(dir_path)\n",
    "    # docs 存放加载之后的纯文本对象\n",
    "    docs = []\n",
    "    # 遍历所有目标文件\n",
    "    for one_file in tqdm(file_lst):\n",
    "        file_type = one_file.split('.')[-1]\n",
    "        if file_type == 'md':\n",
    "            loader = UnstructuredMarkdownLoader(one_file)\n",
    "        elif file_type == 'txt':\n",
    "            loader = TextLoader(one_file)\n",
    "        elif file_type == 'csv':\n",
    "            loader = CSVLoader(one_file)\n",
    "        else:\n",
    "            # 如果是不符合条件的文件，直接跳过\n",
    "            continue\n",
    "        docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5198502f-3f2f-4d3b-acc3-1370350d97a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4795/4795 [00:03<00:00, 1573.06it/s]\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /root/model/jinaai/jina-embeddings-v2-base-zh and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# 构建向量数据库\\n# 定义持久化路径\\npersist_directory = 'data_base/vector_db/chroma'\\n# 加载数据库\\nvectordb = Chroma.from_documents(\\n    documents=split_docs,\\n    embedding=embeddings,\\n    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\\n)\\n# 将加载的向量数据库持久化到磁盘上\\nvectordb.persist\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 目标文件夹\n",
    "tar_dir = [\n",
    "    #\"/root/project/TinyRAG/data/2024-01-23_10-07-23\",\n",
    "    \"/root/project/bisai2/data/vector_db_txt\",\n",
    "]\n",
    "\n",
    "# 加载目标文件\n",
    "docs = []\n",
    "for dir_path in tar_dir:\n",
    "    docs.extend(get_text(dir_path))\n",
    "\n",
    "\n",
    "# 对文本进行分块\n",
    "# Todo  怎么查看超长的chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=150)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/root/model/jinaai/jina-embeddings-v2-base-zh\")\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"/root/data/model/sentence-transformer\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectordb = FAISS.from_documents(documents, embeddings)\n",
    "# 将加载的向量数据库持久化到磁盘上\n",
    "vectordb.save_local('./vectordb')\n",
    "\n",
    "'''\n",
    "# 构建向量数据库\n",
    "# 定义持久化路径\n",
    "persist_directory = 'data_base/vector_db/chroma'\n",
    "# 加载数据库\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    ")\n",
    "# 将加载的向量数据库持久化到磁盘上\n",
    "vectordb.persist()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79c462-f5b9-442b-95f6-a84bd571f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载已有数据库\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/root/model/jinaai/jina-embeddings-v2-base-zh\")\n",
    "vectordb = FAISS.load_local('./vectordb',embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5615e218-42d9-4ea1-b377-8f1b7c318896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*关于上汽集团的董事长，我需要查询一下相关信息。根据最新的信息，上汽集团的董事长是陈虹。他在2018年5月25日出任该职务，并在之后一直担任该职位。陈虹在汽车行业中拥有丰富的经验和卓越的领导能力，他在上汽集团的任职期间推动了一系列改革和发展措施，进一步提升了公司的竞争力。 \n"
     ]
    }
   ],
   "source": [
    "# 建立一个链，该链接受问题和检索到的文档并生成答案\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"使用以上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "如果给定的上下文无法让你做出回答，请对回答的内容以“*”开头，然后根据你的理解作答。\n",
    "有用的回答: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "#对于给定的问题，我们可以使用检索器动态选择最相关的文档并将其传递进去\n",
    "\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# 返回结果\n",
    "response = retrieval_chain.invoke({\"input\": \"上汽集团的董事长是谁\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "add1af35-84d8-4c5f-a7aa-e06477eafb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /root/model/jinaai/jina-embeddings-v2-base-zh and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 加载文本\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# 加载嵌入模型\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/root/model/jinaai/jina-embeddings-v2-base-zh\")\n",
    "\n",
    "#pip install faiss-cpu\n",
    "\n",
    "# 建立索引\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaaff2f2-50ce-4105-a275-c2c8876d9835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /root/model/jinaai/jina-embeddings-v2-base-zh and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing in several ways:\n",
      "\n",
      "1. **Beta Testing**: LangSmith allows developers to collect more data on how their LLM applications are performing in real-world scenarios. It helps in understanding the types of inputs the app is performing well or poorly on and how it's breaking down in those cases.\n",
      "\n",
      "2. **Collecting Feedback**: LangSmith enables users to attach feedback scores to logged traces, which can be hooked up to a feedback button in the app. This helps in gathering human feedback on the responses produced by the application and highlighting edge cases causing problematic responses.\n",
      "\n",
      "3. **Annotating Traces**: LangSmith supports sending runs to annotation queues, allowing annotators to closely inspect interesting traces and annotate them with respect to different criteria. This helps in catching regressions across important evaluation criteria.\n",
      "\n",
      "4. **Adding Runs to a Dataset**: As the application progresses through the beta testing phase, LangSmith enables users to add runs as examples to datasets, expanding test coverage on real-world scenarios. This is beneficial in refining and improving the application's performance.\n",
      "\n",
      "5. **Monitoring and A/B Testing**: LangSmith provides monitoring charts to track key metrics over time. It also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.\n",
      "\n",
      "Overall, LangSmith provides a comprehensive platform for testing LLM applications, allowing developers to collect data, gather feedback, annotate traces, and monitor performance at various stages of the application development lifecycle. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 建立一个链，该链接受问题和检索到的文档并生成答案\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "#对于给定的问题，我们可以使用检索器动态选择最相关的文档并将其传递进去\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# 返回结果\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9408b769-e9e9-47cb-922d-02a0b0528d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__f438da3457f5446db8509ca0e0ee48f3\"  # Update to your API key\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef95f53-674d-490f-9d88-db99e998614c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf01d7-215b-420e-bd5f-5754b1287722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c713801b-e7db-445d-b6e1-d88c3dcd7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "#from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c10f0f3-1b5c-4270-aa95-b06b2b8294ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f55000cc-7956-417a-930f-21ae30ece02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Read Tracer Session\", \"description\": \"Get a specific session.\"}}}}\n",
      "{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"operationId\": \"read_tracer_session_api_v1_sessions__session_id__get\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks = splitter.split_json(json_data=json_data)\n",
    "# The splitter can also output documents\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "\n",
    "# or a list of strings\n",
    "texts = splitter.split_text(json_data=json_data)\n",
    "\n",
    "print(texts[0])\n",
    "print(texts[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96eeb8e-e75e-4eb1-943f-c9f4d4f578aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internlm-demo",
   "language": "python",
   "name": "internlm-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
