{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0cc994-e5c5-4195-bc51-993d997afd2f",
   "metadata": {},
   "source": [
    "# 步骤一、加载环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1b2ce2-2c25-4076-9558-d2ae3b4a31fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "#set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52596125-7a6a-496c-b7a1-5a17d38ddc3e",
   "metadata": {},
   "source": [
    "# 步骤二、加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7b464c-da69-46b2-a81a-4880b15cb1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc221630bc3242d28d6de4cc07ceec5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_7b_path = '/root/share/model_repos/internlm2-chat-7b'\n",
    "#model_7b_path = '/root/project/bisai2/sft-guanfang-self/merged_7b_e10'\n",
    "model_7b_path = '/root/model/internlm2-chat-7b-wendongmi'\n",
    "tokenizer_7b = AutoTokenizer.from_pretrained(model_7b_path, trust_remote_code=True)\n",
    "model_7b = AutoModelForCausalLM.from_pretrained(model_7b_path, device_map=\"auto\", trust_remote_code=True, load_in_4bit=True)\n",
    "model_7b = model_7b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ae21c0-f85d-4499-aa6c-cc88d5817554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'您好，我是一款基于人工智能技术的智能助手，致力于通过语言模型为用户提供服务。 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, history = model_7b.chat(tokenizer_7b, \"请你做一个自我介绍\", history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c7b9cb-0b34-4ec0-a06a-81ea996314e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fb23edba3b49389938a9c3ca494eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_20b_path = '/root/share/model_repos/internlm2-chat-20b'\n",
    "tokenizer_20b = AutoTokenizer.from_pretrained(model_20b_path, trust_remote_code=True)\n",
    "model_20b = AutoModelForCausalLM.from_pretrained(model_20b_path, device_map=\"auto\", trust_remote_code=True, load_in_4bit=True)\n",
    "model_20b = model_20b.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4a992-f34a-4884-8365-e6b3dbe977e2",
   "metadata": {},
   "source": [
    "# 步骤三、初始化工具类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e022f2-4797-453f-9b64-5c9f91639f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取字符串\n",
    "def extract_xuanxiang(text):\n",
    "    # 给定的字符串\n",
    "    \n",
    "    # 使用正则表达式匹配\"正确选项是：\"和\"。\"之间的内容\n",
    "    pattern = r\"正确选项是：([^。]+)\"\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    # 如果找到匹配项，提取并打印内容\n",
    "    if match:\n",
    "        extracted_content = match.group(1)\n",
    "        return extracted_content\n",
    "    else:\n",
    "        print(\"没有找到匹配的内容\")\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349ef4f-09d0-4bfe-9648-3736e12544ee",
   "metadata": {},
   "source": [
    "# 步骤四、做多选题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ac156d-d9d0-4b8e-9dff-6ddb777d9058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "answer: C\n",
      "1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "answer: A\n",
      "2---\n",
      "answer: A、B\n",
      "3---\n",
      "answer: A、C\n",
      "4---\n",
      "answer: A\n",
      "5---\n",
      "answer: A、B、C\n",
      "6---\n",
      "answer: B\n",
      "7---\n",
      "answer: A、C、D\n",
      "8---\n",
      "answer: A、B、C、D\n",
      "9---\n",
      "answer: A、B\n",
      "10---\n",
      "answer: \n",
      "11---\n",
      "answer: A、B、C、D\n",
      "12---\n",
      "answer: \n",
      "13---\n",
      "answer: B\n",
      "14---\n",
      "answer: B\n",
      "15---\n",
      "answer: D\n",
      "16---\n",
      "answer: A\n",
      "17---\n",
      "answer: A、B、C、D\n",
      "18---\n",
      "answer: A\n",
      "19---\n",
      "answer: C\n",
      "20---\n",
      "answer: A、B、C、D\n",
      "21---\n",
      "answer: C、D\n",
      "22---\n",
      "answer: A、B、C、D\n",
      "23---\n",
      "answer: A\n",
      "24---\n",
      "answer: C\n",
      "25---\n",
      "answer: C\n",
      "26---\n",
      "answer: C\n",
      "27---\n",
      "answer: C\n",
      "28---\n",
      "answer: B、D\n",
      "29---\n",
      "answer: B、C\n",
      "30---\n",
      "answer: A、B、C、D\n",
      "31---\n",
      "answer: C\n",
      "32---\n",
      "answer: A、B、C\n",
      "33---\n",
      "answer: B、C\n",
      "34---\n",
      "answer: A、C、D\n",
      "35---\n",
      "answer: B\n",
      "36---\n",
      "answer: D\n",
      "37---\n",
      "answer: A、B、C、D\n",
      "38---\n",
      "answer: A、C、D\n",
      "39---\n",
      "answer: A、B、C、D\n",
      "40---\n",
      "answer: A、C\n",
      "41---\n",
      "answer: A、C、D\n",
      "42---\n",
      "answer: B\n",
      "43---\n",
      "answer: A、B、D\n",
      "44---\n",
      "answer: C\n",
      "45---\n",
      "answer: A、D\n",
      "46---\n",
      "answer: A、B、C、D\n",
      "47---\n",
      "answer: A、B、C、D\n",
      "48---\n",
      "answer: D\n",
      "49---\n",
      "answer: A、C\n",
      "50---\n",
      "answer: A\n",
      "51---\n",
      "answer: D\n",
      "52---\n",
      "answer: A、B\n",
      "53---\n",
      "answer: A\n",
      "54---\n",
      "answer: A、B、D\n",
      "55---\n",
      "answer: B\n",
      "56---\n",
      "answer: A、C\n",
      "57---\n",
      "answer: C、D\n",
      "58---\n",
      "answer: C\n",
      "59---\n",
      "answer: C、D\n",
      "60---\n",
      "answer: A、B、C、D\n",
      "61---\n",
      "answer: A、B、C、D\n",
      "62---\n",
      "answer: A\n",
      "63---\n",
      "answer: B\n",
      "64---\n",
      "answer: B\n",
      "65---\n",
      "answer: B\n",
      "66---\n",
      "answer: A\n",
      "67---\n",
      "answer: A\n",
      "68---\n",
      "answer: A\n",
      "69---\n",
      "answer: A、C\n",
      "70---\n",
      "answer: A、B\n",
      "71---\n",
      "answer: A、D\n",
      "72---\n",
      "answer: A、B、C\n",
      "73---\n",
      "answer: C\n",
      "74---\n",
      "answer: B、C、D\n",
      "75---\n",
      "answer: B\n",
      "76---\n",
      "answer: B、D\n",
      "77---\n",
      "answer: B\n",
      "78---\n",
      "answer: B、C、D\n",
      "79---\n",
      "answer: B、C\n",
      "80---\n",
      "answer: A、B、C、D\n",
      "81---\n",
      "answer: A、C\n",
      "82---\n",
      "answer: A、B、C、D\n",
      "83---\n",
      "answer: A、B、C、D\n",
      "84---\n",
      "answer: B、C、D\n",
      "85---\n",
      "answer: A、B、C、D\n",
      "86---\n",
      "answer: A、B\n",
      "87---\n",
      "answer: A、B、C、D\n",
      "88---\n",
      "answer: A、B、D\n",
      "89---\n",
      "answer: A、C、D\n",
      "90---\n",
      "answer: D\n",
      "91---\n",
      "answer: A、C\n",
      "92---\n",
      "answer: A、B\n",
      "93---\n",
      "answer: A、C\n",
      "94---\n",
      "answer: A、D\n",
      "95---\n",
      "answer: D\n",
      "96---\n",
      "answer: A、B、C、D\n",
      "97---\n",
      "answer: D\n",
      "98---\n",
      "answer: A\n",
      "99---\n",
      "answer: C\n",
      "CSV文件保存成功！\n"
     ]
    }
   ],
   "source": [
    "#选择题评测\n",
    "#设置\n",
    "do_sample = False\n",
    "temperature = 0.1 \n",
    "top_p = 0.3\n",
    "meta_instruction = '你是上市公司的董秘，你乐于助人，诚实无害，你竭诚为投资者解答关于公司运营、财务状况、投资者关系等方面的问题。'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#df = pd.DataFrame(pd.read_csv('name.csv',header=1))\n",
    "# 读取xls文件\n",
    "df = pd.read_excel('./data/gong_gao_ce_ping_you_xuan_xiang.xls')\n",
    "\n",
    "df.loc[:, '大模型答案'] = \"\"\n",
    "df.loc[:, '大模型回复'] = \"\"\n",
    "for index, row in df.iterrows():\n",
    "    #if index > 2:\n",
    "    #    break\n",
    "    \n",
    "    print(index, end=\"\")\n",
    "    prompt = f\"\"\"{row['评测问题']}\n",
    "\n",
    "请你围绕问题题干并忠实于公告原文，从下面的选项中筛选出正确的选项，你的回复里面只保留正确的选项：    \n",
    "选项A：{row['选项A']}\n",
    "选项B：{row['选项B']}\n",
    "选项C：{row['选项C']}\n",
    "选项D：{row['选项D']}\n",
    "\"\"\"\n",
    "    \n",
    "    response1, history = model_20b.chat(tokenizer_20b, prompt, history=[],do_sample=do_sample,temperature=temperature,top_p=top_p,meta_instruction=meta_instruction)\n",
    "    response1, history = model_20b.chat(tokenizer_20b, prompt, history=[],do_sample=do_sample,meta_instruction=meta_instruction)\n",
    "\n",
    "    #answer = extract_xuanxiang(response)\n",
    "    #print(response1)\n",
    "    print(\"---\")\n",
    "    #print(f\"response1: {response1}\")\n",
    "    prompt = f\"\"\"{response1}\n",
    "    从以上多项选择题的题解中提取出正确的答案选项，不要对选项进行分析，答案用“、”分隔，正确的选项是：\n",
    "\"\"\"\n",
    "    answer_list = []\n",
    "    response2, history = model_20b.chat(tokenizer_20b, prompt, history=[],do_sample=do_sample,meta_instruction=meta_instruction)\n",
    "    #print(f\"response2: {response2}\")\n",
    "    if 'A' in response2:\n",
    "        answer_list.append(\"A\")\n",
    "    if 'B' in response2:\n",
    "        answer_list.append(\"B\")\n",
    "    if 'C' in response2:\n",
    "        answer_list.append(\"C\")\n",
    "    if 'D' in response2:\n",
    "        answer_list.append(\"D\")\n",
    "\n",
    "    answer = \"、\".join(answer_list)\n",
    "    print(f\"answer: {answer}\")\n",
    "    #response = \"da mo xing hui da\" + str(index)\n",
    "    df.at[index, '大模型答案'] = answer\n",
    "    df.at[index, '大模型回复'] = response1\n",
    "    \n",
    "filename = '/root/project/bisai2/sft-guanfang/gong_gao_ce_ping_you_xuan_xiang_多选_result_.csv'\n",
    "\n",
    "# 保存数据到CSV文件\n",
    "df.to_csv(filename, index=False)\n",
    " \n",
    "print('CSV文件保存成功！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d5b1e-e563-40ce-888a-635dd002ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "0---\n",
    "response1: 根据公告内容，我们可以分析出以下信息：\n",
    "\n",
    "1. **沈晓苏先生因到龄退休，已递交辞职申请，辞去公司第八届监事会主席、监事职务。**\n",
    "2. **张小龙先生被推举为公司第八届监事会召集人。**\n",
    "\n",
    "现在，让我们根据这些信息来分析每个选项：\n",
    "\n",
    "**选项A - “体现了公司对年轻化领导力的倾向，有助于引入新思维和管理模式”**：\n",
    "这个选项提出了一个可能的积极影响，即引入新的领导力和管理模式。然而，公告中并没有明确提到年轻化或新思维的引入，因此我们不能确定这是否是公司决策的意图。\n",
    "\n",
    "**选项B - “可能会引起公司内部的权力重组和人事变动，影响短期内的稳定性”**：\n",
    "这个选项提出了一个可能的负面影响，即人事变动可能带来的短期不稳定。虽然公告中提到了人事变动（沈晓苏先生的退休和张小龙先生的推举），但并没有足够的信息来确定这将如何影响公司的短期稳定性。\n",
    "\n",
    "**选项C - “显示了公司对资深员工的尊重和感激，有利于提高员工的归属感和忠诚度”**：\n",
    "这个选项提出了一个积极的方面，即公司对退休员工的尊重和感激可能提高员工的归属感和忠诚度。公告中确实提到了对沈晓苏先生贡献的肯定和感谢，这可能对员工产生积极影响。\n",
    "\n",
    "**选项D - “可能会导致监事会与董事会之间权力平衡的变化，影响决策效率”**：\n",
    "这个选项提出了一个可能的负面影响，即权力平衡的变化可能影响决策效率。公告中没有提及任何关于权力平衡变化的信息，因此我们无法确定这是否是一个实际的问题。\n",
    "\n",
    "综上所述，选项C“显示了公司对资深员工的尊重和感激，有利于提高员工的归属感和忠诚度”是最符合公告内容的分析，因为它直接与公告中提到的对沈晓苏先生贡献的肯定和感谢相呼应。因此，正确答案是C。 \n",
    "response2: C \n",
    "answer: C\n",
    "1---\n",
    "response1: 根据公告内容，我们可以分析出以下信息：\n",
    "\n",
    "1. **股权激励计划设计合理，能够明确激励对象，提高激励效率**：公告中明确了股权激励计划是根据相关法律法规和股东大会授权进行的，且授予对象是符合授予条件的28名激励对象。这表明激励计划的设计是符合规定的，并且授予对象是经过筛选的。\n",
    "\n",
    "2. **授予限制性股票的数量可能过多，导致公司股权过度稀释**：公告中提到授予了546.5万股限制性股票，这的确是一个较大的数量。然而，公告中并未提供公司的总股本信息，因此无法直接判断这个数量是否会导致股权过度稀释。\n",
    "\n",
    "3. **激励对象的选择标准不明确，可能引起内部不公平和矛盾**：公告中并未详细说明激励对象的选择标准，只是提到授予了28名符合条件的激励对象。这可能使得一些员工对选择标准产生疑问，从而引发不公平感。\n",
    "\n",
    "4. **限制性股票授予的条件和解锁标准设置得当，有利于激励对象长期贡献**：公告中提到限制性股票的授予和解锁条件是由公司董事会决定的，但具体条件并未在公告中明确列出。因此，我们无法判断这些条件是否得当。\n",
    "\n",
    "综上所述，选项A“股权激励计划设计合理，能够明确激励对象，提高激励效率”是正确的选项，因为公告中明确了股权激励计划是根据相关法律法规和股东大会授权进行的，且授予对象是符合授予条件的28名激励对象。其他选项要么缺乏足够信息（如B和D），要么与公告内容不符（如C）。因此，正确答案是A。 \n",
    "response2: A、股权激励计划设计合理，能够明确激励对象，提高激励效率 \n",
    "answer: A\n",
    "2---\n",
    "response1: 根据公告内容，我们可以分析出回购股份用于员工持股计划或股权激励计划对公司内部管理的潜在影响。\n",
    "\n",
    "选项A：可以提升员工的归属感和忠诚度，增加他们对公司未来发展的信心。\n",
    "选项B：股权激励可以激发员工的工作热情和创新能力，提高公司整体的竞争力。\n",
    "选项C：长期持有要求可能导致员工对股价波动过度敏感，影响其工作表现。\n",
    "选项D：实施股权激励计划可能会增加公司的管理成本和复杂性。\n",
    "\n",
    "结合公告内容，我们可以分析出：\n",
    "\n",
    "- 选项A和B都是正面的影响。回购股份用于员工持股计划或股权激励计划可以提升员工的归属感和忠诚度，增加他们对公司未来发展的信心，同时股权激励可以激发员工的工作热情和创新能力，提高公司整体的竞争力。\n",
    "- 选项C是一个潜在的中性或负面影响。长期持有要求可能导致员工对股价波动过度敏感，这可能会影响其工作表现，但公告中并未提及长期持有要求，因此这个选项可能并不适用。\n",
    "- 选项D是一个潜在的负面影响。实施股权激励计划可能会增加公司的管理成本和复杂性，但公告中并未提及具体的管理成本和复杂性，因此这个选项可能并不适用。\n",
    "\n",
    "综上所述，根据公告内容，选项A和B是正确的选项，因为它们描述了回购股份用于员工持股计划或股权激励计划的正面影响。选项C和D虽然可能存在，但根据公告内容，它们并不是最直接和明确的影响。因此，正确选项是A和B。 \n",
    "response2: A、B \n",
    "answer: A、B\n",
    "CSV文件保存成功！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6049389-c98d-40fb-9c69-47035e91fefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2131f-df1e-4eb8-89da-f1363bfd320a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93f28b08-b15b-4176-9692-f11cf69e7509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0response1: A：可以提升员工的归属感和忠诚度，增加他们对公司未来发展的信心。\n",
      "B：股权激励可以激发员工的工作热情和创新能力，提高公司整体的竞争力。\n",
      "D：实施股权激励计划可能会增加公司的管理成本和复杂性。 \n",
      "answer: C\n",
      "1response1: A：可以提升员工的归属感和忠诚度，增加他们对公司未来发展的信心。\n",
      "B：股权激励可以激发员工的工作热情和创新能力，提高公司整体的竞争力。\n",
      "D：实施股权激励计划可能会增加公司的管理成本和复杂性。 \n",
      "answer: A、D\n",
      "2response1: A：可以提升员工的归属感和忠诚度，增加他们对公司未来发展的信心。\n",
      "B：股权激励可以激发员工的工作热情和创新能力，提高公司整体的竞争力。\n",
      "D：实施股权激励计划可能会增加公司的管理成本和复杂性。 \n",
      "answer: A、B、D\n",
      "3response1: A：可以提升员工的归属感和忠诚度，增加他们对公司未来发展的信心。\n",
      "B：股权激励可以激发员工的工作热情和创新能力，提高公司整体的竞争力。\n",
      "D：实施股权激励计划可能会增加公司的管理成本和复杂性。 \n",
      "answer: A、B、C、D\n",
      "4response1: A：可以提升员工的归属感和忠诚度，增加他们对公司未来发展的信心。\n",
      "B：股权激励可以激发员工的工作热情和创新能力，提高公司整体的竞争力。\n",
      "D：实施股权激励计划可能会增加公司的管理成本和复杂性。 \n",
      "answer: A、B、D\n",
      "5"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(index, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m评测问题\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m选项A：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m选项A\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m选项B：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m选项B\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m3、从“与题干的相关程度、原文对选项的支持程度、选项的合理性”等方面对每个选项进行分析总结\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     response, history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_20b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_20b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     answer \u001b[38;5;241m=\u001b[39m extract_xuanxiang(response)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/internlm2-chat-20b/modeling_internlm2.py:1167\u001b[0m, in \u001b[0;36mInternLM2ForCausalLM.chat\u001b[0;34m(self, tokenizer, query, history, streamer, max_new_tokens, do_sample, temperature, top_p, meta_instruction, **kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;66;03m# also add end-of-assistant token in eos token id to avoid unnecessary generation\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m eos_token_id \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39meos_token_id, tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|im_end|>\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m-> 1167\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]) :]\n\u001b[1;32m   1178\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2734\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#选择题评测\n",
    "#提示词按   问题  选项   要求来组织\n",
    "#效果不好  不能严格按格式来回复  temperature: float = 0.8, top_p: float = 0.8, meta_instruction:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "temperature = 0.1 \n",
    "top_p = 0.8\n",
    "meta_instruction = '你是上市公司的董秘，你乐于助人，诚实无害，你竭诚为投资者解答关于公司运营、财务状况、投资者关系等方面的问题。'\n",
    "\n",
    "#df = pd.DataFrame(pd.read_csv('name.csv',header=1))\n",
    "# 读取xls文件\n",
    "df = pd.read_excel('./data/gong_gao_ce_ping_you_xuan_xiang.xls')\n",
    "\n",
    "df.loc[:, '大模型答案'] = \"\"\n",
    "df.loc[:, '大模型回复'] = \"\"\n",
    "for index, row in df.iterrows():\n",
    "    print(index, end=\"\")\n",
    "    prompt = f\"\"\"{row['评测问题']}\n",
    "选项A：{row['选项A']}\n",
    "选项B：{row['选项B']}\n",
    "选项C：{row['选项C']}\n",
    "选项D：{row['选项D']}\n",
    "\n",
    "\n",
    "请你从上面的选项中选择正确的选项，正确的选项可能不止一个，你要围绕问题题干并忠实于公告原文进行分析\n",
    "按以下格式组织回答：\n",
    "1、假如答案只有C，按格式：“正确选项是：C。”给出答案。假如答案A、B、C都对，则格式“正确选项是：A、B、C。”给出答案\n",
    "2、按格式：“A:4,B:3,C:9,D:6”对每个选项进行把握度打分，分数从0到10，分数越高，表示把握越大，不要分析，只给结果\n",
    "3、从“与题干的相关程度、原文对选项的支持程度、选项的合理性”等方面对每个选项进行分析总结\n",
    "\"\"\"\n",
    "    \n",
    "    response, history = model_20b.chat(tokenizer_20b, prompt, history=[])\n",
    "    answer = extract_xuanxiang(response)\n",
    "    print(f\"response1: {response1}\")\n",
    "    print(f\"answer: {answer}\")\n",
    "    #response = \"da mo xing hui da\" + str(index)\n",
    "    df.at[index, '大模型答案'] = answer\n",
    "    df.at[index, '大模型回复'] = response\n",
    "    \n",
    "filename = '/root/project/bisai2/sft-guanfang/gong_gao_ce_ping_you_xuan_xiang_多选_result_.csv'\n",
    "\n",
    "# 保存数据到CSV文件\n",
    "df.to_csv(filename, index=False)\n",
    " \n",
    "print('CSV文件保存成功！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42947b0f-cb2b-4b50-9561-7eae1527f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择题评测\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#df = pd.DataFrame(pd.read_csv('name.csv',header=1))\n",
    "# 读取xls文件\n",
    "df = pd.read_excel('./data/gong_gao_ce_ping_you_xuan_xiang.xls')\n",
    "\n",
    "df.loc[:, '大模型答案'] = \"\"\n",
    "df.loc[:, '大模型回复'] = \"\"\n",
    "for index, row in df.iterrows():\n",
    "    print(index, end=\"\")\n",
    "    prompt = f\"\"\"{row['评测问题']}\n",
    "\n",
    "请你围绕问题题干并忠实于公告原文，从下面的选项中筛选出正确的选项，你的回复里面只保留正确的选项：    \n",
    "选项A：{row['选项A']}\n",
    "选项B：{row['选项B']}\n",
    "选项C：{row['选项C']}\n",
    "选项D：{row['选项D']}\n",
    "\"\"\"\n",
    "    \n",
    "    response1, history = model_20.chat(tokenizer20, prompt, history=[])\n",
    "    #answer = extract_xuanxiang(response)\n",
    "    #print(response1)\n",
    "    print(\"---\")\n",
    "    prompt = f\"\"\"{response1}\n",
    "    从以上多项选择题的题解中提取出正确的答案选项，不要对选项进行分析，答案用“、”分隔，正确的选项是：\n",
    "\"\"\"\n",
    "    answer_list = []\n",
    "    response2, history = model20.chat(tokenizer20, prompt, history=[])\n",
    "    print(response2)\n",
    "    if 'A' in response2:\n",
    "        answer_list.append(\"A\")\n",
    "    if 'B' in response2:\n",
    "        answer_list.append(\"B\")\n",
    "    if 'C' in response2:\n",
    "        answer_list.append(\"C\")\n",
    "    if 'D' in response2:\n",
    "        answer_list.append(\"D\")\n",
    "\n",
    "    answer = \"、\".join(answer_list)\n",
    "    print(\"---+\")\n",
    "    print(answer)\n",
    "    print(\"---+++\")\n",
    "    #response = \"da mo xing hui da\" + str(index)\n",
    "    df.at[index, '大模型答案'] = answer\n",
    "    df.at[index, '大模型回复'] = response1\n",
    "    \n",
    "filename = '/root/project/bisai2/sft-guanfang/gong_gao_ce_ping_you_xuan_xiang_多选_result_.csv'\n",
    "\n",
    "# 保存数据到CSV文件\n",
    "df.to_csv(filename, index=False)\n",
    " \n",
    "print('CSV文件保存成功！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internlm-demo",
   "language": "python",
   "name": "internlm-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
